<!-- Improved compatibility of back to top link: See: https://github.com/othneildrew/Best-README-Template/pull/73 -->
<a id="readme-top"></a>

<!-- PROJECT SHIELDS -->
<!--
*** I'm using markdown "reference style" links for readability.
*** Reference links are enclosed in brackets [ ] instead of parentheses ( ).
*** See the bottom of this document for the declaration of the reference variables
*** for contributors-url, forks-url, etc. This is an optional, concise syntax you may use.
*** https://www.markdownguide.org/basic-syntax/#reference-style-links
-->




<!-- PROJECT LOGO -->
<br />
<div align="center">
  </a>

  <h3 align="center">The Generative AI Image Consistency Experiment</h3>

  <p align="center">
    This experiment explores consistency between complex characters and environments in image generation through the use of user defined prompts, prompt narrowing, prompt flooding, and prompt referencing.
    <br />
    Prompt Referencing Application
    <br />
    <br />
    <br />
    <a href="https://britainthomas32.wixsite.com/portfolio/projects">Britain Thomas</a>
    &middot;
    <a href="https://sites.google.com/view/viza626/home">VIZA 626</a>
  </p>
</div>

[![4-comma][images-fig1]](https://example.com)

Figure 1. The Final result of the application of all five phases.

<!-- Abstract -->
## Abstract

The Generative AI Image Consistency Experiment focused on applying the concept of user defined prompts, prompt narrowing, prompt flooding, and prompt referencing. The application of these series of steps to produce consistent generated images, allowed for more cohesive storyboard creation with consistent characters and environments. The goal of combining these approaches was to simplify and speed up the storyboard creation process for more logical consistency between panels in the form of a comic. The application of user defined prompts proved significant in mitigating anomalies and illusions produced by the AI, and by applying prompt referencing along with this method, allowed for consistency between panels. Applying the concept of image-to-image based prompting, gave more successful results than traditional text-to-image only prompts.


[![4-comma][images-fig2]](https://example.com)

Figure 2 is the Experiment Workflow to generate the consistency comic through the use of the five phases of the experiment through user defined prompts, prompt narrowing, prompt flooding, prompt referencing to then be assembled in the the finalization process seen in Phase 5.


<!-- Introduction and Related Works -->
## Introduction and Related Works

Consistent characters and environments give access to more application of AI assisted storyboard creation, the concepts of applying user defined prompts, prompt narrowing, prompt flooding, and prompt referencing developed the way for consistent board creation. Traditional methods that do not utilize these series of steps and utilize some of the concepts in isolation show a room for improvement [1]. The benefit of OpenAi’s DALL-E 3.0 model allows for the combination of all these steps to improve image quality being a transformer model when combined with ChatGPT o4 prompt generation. DALL-E 3.0’s image generation suffers from inaccuracy, but maintains artistic abstraction between tests with the methods of combining text and image generation [2]; therefore, the benefit of combining the methods gives more room for appealing images at the cost of accuracy, but with the introduction the series of steps in the Generative AI Image Consistency Experiment paves way to consistency of these appealing artistic images once defined by the user and saved in memory. The benefit of DALL-E is its incorporation of ChatGPT and the functionality shared between the two AI models, because of this integration and the introduction of DALL-E 3.0 the quality of images the model produces are higher [3]. With the introduction of user defined prompts, once saved into memory, utilized in this experiment gave an increase in consistency while maintaining quality between the comic panels.


[![4-comma][images-fig3]](https://example.com)

Figure 3 is the Experiment Pipeline used to generate the comic's consistency for each of the ten panels showcasing the steps of the entire experiment in a condensed view.


[![4-comma][images-fig4]](https://example.com)

Figure 4 is the process of user prompt referencing replacing the area defined by the user to place the characters in the final environment background during Phase 4 of the experiment.


[![4-comma][images-fig5]](https://example.com)

Figure 5 is the mathematics formula for user prompt defining during Phase 1 of the experiment. The user defined prompts (U) are combined with style referenced images (S) to generate images that match the users defined prompt (G). The character is then generated with several views (C) in a set (n) of varying angles. The environment is then generated with several views (E) in a set (n) of varying angles. Then the set of images is combined to save in memory as the users prompt as a set of all images (I).


[![4-comma][images-fig6]](https://example.com)

Figure 6 is the mathematics formula for prompt narrowing during Phase 2 of the experiment. Each initial prompts generated (p) are first narrowed down by color (c) and theme (t). The the initial prompts are influenced by the narrative (N) to further direct the image generation consistency.


[![4-comma][images-fig7]](https://example.com)

Figure 7 is the mathematics formula for prompt flooding during Phase 3 of the experiment. The best prompts (p*) are determined by the highest evaluation score (arg max) of the evaluated (E) generated images (G) when generated with the initial prompts (p). Where the initial prompts (p) are a series of iterated colors (c) and themes (t). Prompt Flooding (ph) is then enriched by the combination of all elements of prompts influence of the final prompt (P).


## Methodology

The Generative AI Image Consistency Experiment consisted of a five phase process to generate each image within the comic, seen in figure 1. The workflow of the entire experiment, figure 2, showcases each step in creating the individual panels, while the pipeline is shown in a condensed format seen in figure 3. The process to enrich the desirable images seen in figure 1 are showcased in figure 4 of the process in action. Phase 1 of the experiment consisted of providing images created by the user to associate keywords for DALL-E to identify what the user is referring to when a prompt is stated by the user [figure 5]. Phase 2 of the experiment consisted of prompt narrowing to optimize prompts to its associated user prompt to produce the most successful and desirable output for the user, and each prompt is then saved in memory with its associated images [figure 6]. Phase 3 of the experiment consisted of prompt flooding, which takes the saved memory user defined and narrowed prompts to then hyper specify what the desirable image is, this is to mitigate illusions between future iterations in the prompt enrichment process seen in prompt flooding [figure 7]. Phase 4 of the experiment consisted of prompt referencing, which takes the final images and tweaks them for edits defined by the user for more desirable images [figure 8]. Phase 5 is the finalization step of the experiment where all images are then combined to fit a narrative finalizing the storyboard of the comic [figure 9 & 10]. The final panel creation process is showcased in figure 11, along with all the edits and modifications needed to produce a final panel.


[![4-comma][images-fig8]](https://example.com)

Figure 8 is the mathematics formula for user prompt defining during Phase 4 of the experiment. The user defined series of images(Iu) is unioned with the defined character images (C) and its variants (V) and unioned along with the defined environment (E) and its variants (V). The final unioned image (ph) is then combined with reference images (R) to tweak the final generated image result (I`).


[![4-comma][images-fig9]](https://example.com)

Figure 9 is the mathematics formula for user prompt defining during Phase 5 of the experiment. The final images are then iterated until the best generated image result is achieved (Itop) based on the top ten images (T10) of the final tweaked image (I`). The comic is then assembled based on the most successful results.


[![4-comma][images-fig10]](https://example.com)

Figure 10 is the mathematics formula for the experiment to produce every stage of the experimental workflow to generate each comic panel.


[![4-comma][images-fig11]](https://example.com)

Figure 11 is the Panel Creation Process used in the final comic, and contains each iteration to produce the final panel.




## Result and Future Work

The final result of the experiment proved a successful strategy to generate consistent comic panels for future and higher quality storyboard creation. The introduction of user defined prompts seen in figure 3, gave editing and tweaking the final image easier when anomalies did occur. Without user defined prompts, the correcting process of images that had anomalies would drastically reduce desirability: This was showcased in previous experiments that utilized all of these methods in isolation or without this experiment's full workflow. The original vision of this experiment was consistency between characters and environment, but the introduction and effectiveness of user defined prompts paved the way for the future experiment of AI assisted image editing. The ability to remove anomalies by defining what specifically needs modifying by circling affected areas, similar to how art directors suggest editing scenes in 3D work, gives the ability more flexibility to fix anomalies and illusions within AI generated artwork. Potential room for improvement of this experiment is in the dataset of the user defined prompts, this area could vastly improve when introducing successfully generated images to train DALL-E tailored to the user, and their own bias. 



## Conclusion
The overall experiment proved to be effective in producing successful image consistency along with correcting anomalies when they did occur. By incorporating user defined prompts, allowed to match the bias between user and DALL-E to reduce illusions when generating images. Prompt narrowing, prompt flooding, and prompt referencing allowed the maximization of the incorporation between DALL-E and ChatGPT, and the introduction of user defined prompts, gave way to mitigate and correct illusions. The goal of this experiment was to showcase the combination of text-to-image and image-to-image referencing that gives OpenAi’s image generation model leverage over other competitors; however, when these methods are done in isolation they show the platforms limitations with illusions, bias, and inconsistency: When following this experiments workflow, the issues are significantly reduced.



<!-- Bibliography -->
## Bibliography 
[1] Yuan, J., Cao, X., Li, C., Yang, F., Lin, J., & Cao, X. (2023). Pku-i2iqa: An image-to-image quality assessment 
database for ai generated images. arXiv preprint arXiv:2311.15556.

[2] Temsah, M. H., Alhuzaimi, A. N., Almansour, M., Aljamaan, F., Alhasan, K., Batarfi, M. A., ... & Nazer, R. (2024). 
Art or artifact: evaluating the accuracy, appeal, and educational value of AI-generated imagery in DALL· E 3 for 
illustrating congenital heart diseases. Journal of Medical Systems, 48(1), 54.

[3] Pooja, M. M., Sulfath, P. M., Sheena, K. M., Pooja, M. M., & Sulfath, P. M. DALL-E 3: Advanced AI image generation 
model.



<!-- CONTACT -->
## Contact

Britain Thomas - britainthomas@tamu.edu

Personal Website: https://britainthomas32.wixsite.com/portfolio/projects




<!-- ACKNOWLEDGMENTS -->
## Acknowledgments

This work is submitted as part of Assignment 3 for the VIZA 626 course at Texas A&M University, under the instruction of Professor You-Jin Kim, during the Spring 2025 semester.

VIZA 626 Class Website: [https://sites.google.com/view/viza626/](https://sites.google.com/view/viza626/home)

<!-- MARKDOWN LINKS & IMAGES -->
<!-- https://www.markdownguide.org/basic-syntax/#reference-style-links -->
[contributors-shield]: https://img.shields.io/github/contributors/othneildrew/Best-README-Template.svg?style=for-the-badge
[contributors-url]: https://github.com/othneildrew/Best-README-Template/graphs/contributors
[forks-shield]: https://img.shields.io/github/forks/othneildrew/Best-README-Template.svg?style=for-the-badge
[forks-url]: https://github.com/othneildrew/Best-README-Template/network/members
[stars-shield]: https://img.shields.io/github/stars/othneildrew/Best-README-Template.svg?style=for-the-badge
[stars-url]: https://github.com/othneildrew/Best-README-Template/stargazers
[issues-shield]: https://img.shields.io/github/issues/othneildrew/Best-README-Template.svg?style=for-the-badge
[issues-url]: https://github.com/othneildrew/Best-README-Template/issues
[license-shield]: https://img.shields.io/github/license/othneildrew/Best-README-Template.svg?style=for-the-badge
[license-url]: https://github.com/othneildrew/Best-README-Template/blob/master/LICENSE.txt
[linkedin-shield]: https://img.shields.io/badge/-LinkedIn-black.svg?style=for-the-badge&logo=linkedin&colorB=555
[linkedin-url]: https://linkedin.com/in/othneildrew
[product-screenshot]: images/screenshot.png
[images-fig1]: images/fig1.png
[images-fig2]: images/fig2.png
[images-fig3]: images/fig3.png
[images-fig4]: images/fig4.png
[images-fig5]: images/fig5.png
[images-fig6]: images/fig6.png
[images-fig7]: images/fig7.png
[images-fig8]: images/fig8.png
[images-fig9]: images/fig9.png
[images-fig10]: images/fig10.png
[images-fig11]: images/fig11.png
[Next.js]: https://img.shields.io/badge/next.js-000000?style=for-the-badge&logo=nextdotjs&logoColor=white
[Next-url]: https://nextjs.org/
[React.js]: https://img.shields.io/badge/React-20232A?style=for-the-badge&logo=react&logoColor=61DAFB
[React-url]: https://reactjs.org/
[Vue.js]: https://img.shields.io/badge/Vue.js-35495E?style=for-the-badge&logo=vuedotjs&logoColor=4FC08D
[Vue-url]: https://vuejs.org/
[Angular.io]: https://img.shields.io/badge/Angular-DD0031?style=for-the-badge&logo=angular&logoColor=white
[Angular-url]: https://angular.io/
[Svelte.dev]: https://img.shields.io/badge/Svelte-4A4A55?style=for-the-badge&logo=svelte&logoColor=FF3E00
[Svelte-url]: https://svelte.dev/
[Laravel.com]: https://img.shields.io/badge/Laravel-FF2D20?style=for-the-badge&logo=laravel&logoColor=white
[Laravel-url]: https://laravel.com
[Bootstrap.com]: https://img.shields.io/badge/Bootstrap-563D7C?style=for-the-badge&logo=bootstrap&logoColor=white
[Bootstrap-url]: https://getbootstrap.com
[JQuery.com]: https://img.shields.io/badge/jQuery-0769AD?style=for-the-badge&logo=jquery&logoColor=white
[JQuery-url]: https://jquery.com 
